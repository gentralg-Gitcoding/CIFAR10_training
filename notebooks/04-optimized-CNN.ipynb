{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8541b05d",
   "metadata": {},
   "source": [
    "# Optimized PyTorch CNN with Optuna\n",
    "\n",
    "In this notebook, we optimize a convolutional neural network (CNN) for the CIFAR-10 dataset using Optuna for hyperparameter tuning.\n",
    "\n",
    "## Notebook set-up\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614cd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Package imports\n",
    "from cifar10_tools.pytorch.evaluation import evaluate_model\n",
    "from cifar10_tools.pytorch.plotting import (\n",
    "    plot_sample_images, plot_learning_curves, \n",
    "    plot_confusion_matrix, plot_class_probability_distributions,\n",
    "    plot_evaluation_curves\n",
    ")\n",
    "from cifar10_tools.pytorch.training import train_model\n",
    "\n",
    "# Suppress Optuna info messages (show only warnings and errors)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(315)\n",
    "np.random.seed(315)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077cc69",
   "metadata": {},
   "source": [
    "### Fixed hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048        # Training images come in 5 batches of 10,000\n",
    "\n",
    "# Optuna settings\n",
    "start_new_study = True   # Clear previous results (if any) and start new study\n",
    "n_trials = 50            # Number of optimization trials\n",
    "n_epochs_per_trial = 100 # Epochs per trial\n",
    "n_epochs_final = 200     # Epochs for final model training\n",
    "\n",
    "# SQLite storage for Optuna (local file)\n",
    "data_dir = Path('../data/pytorch')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "storage_path = data_dir / 'optimized_pytorch_cnn.db'\n",
    "storage_url = f'sqlite:///{storage_path}'\n",
    "\n",
    "# CIFAR-10 class names in class order\n",
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e000f",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess CIFAR-10 data\n",
    "\n",
    "CIFAR-10 contains 32x32 color images (3 channels) across 10 classes. We convert the images to grayscale for this demonstration.\n",
    "\n",
    "### 1.1. Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure data directory exists\n",
    "data_dir = Path('../data/pytorch/cifar10')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data preprocessing: convert to grayscale, tensor, and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load training and test datasets\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Image shape: {train_dataset[0][0].shape}')\n",
    "print(f'Number of classes: {len(class_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e56c0",
   "metadata": {},
   "source": [
    "### 1.2. Visualize sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 10 images from the training dataset\n",
    "fig, axes = plot_sample_images(train_dataset, class_names, nrows=2, ncols=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ceb6f",
   "metadata": {},
   "source": [
    "### 1.2. Create training, validation and testing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and preload entire dataset to device for faster training\n",
    "X_train_full = torch.stack([img for img, _ in train_dataset]).to(device)\n",
    "y_train_full = torch.tensor([label for _, label in train_dataset]).to(device)\n",
    "\n",
    "X_test = torch.stack([img for img, _ in test_dataset]).to(device)\n",
    "y_test = torch.tensor([label for _, label in test_dataset]).to(device)\n",
    "\n",
    "# Split training data into train and validation sets (80/20 split)\n",
    "n_train = int(0.8 * len(X_train_full))\n",
    "indices = torch.randperm(len(X_train_full))\n",
    "\n",
    "X_train = X_train_full[indices[:n_train]]\n",
    "y_train = y_train_full[indices[:n_train]]\n",
    "X_val = X_train_full[indices[n_train:]]\n",
    "y_val = y_train_full[indices[n_train:]]\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}, device: {X_train.device}')\n",
    "print(f'y_train shape: {y_train.shape}, device: {y_train.device}')\n",
    "print(f'X_val shape: {X_val.shape}, device: {X_val.device}')\n",
    "print(f'y_val shape: {y_val.shape}, device: {y_val.device}')\n",
    "print(f'X_test shape: {X_test.shape}, device: {X_test.device}')\n",
    "print(f'y_test shape: {y_test.shape}, device: {y_test.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b5c18",
   "metadata": {},
   "source": [
    "### 1.3. Create `DataLoader()` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ece0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_tensor_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_tensor_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_tensor_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_tensor_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_tensor_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training batches: {len(train_loader)}')\n",
    "print(f'Validation batches: {len(val_loader)}')\n",
    "print(f'Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cd71b",
   "metadata": {},
   "source": [
    "## 2. Define CNN architecture with configurable hyperparameters\n",
    "\n",
    "We create a flexible CNN builder that accepts hyperparameters from Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(\n",
    "    n_conv_blocks: int,\n",
    "    initial_filters: int,\n",
    "    fc_units_1: int,\n",
    "    fc_units_2: int,\n",
    "    dropout_rate: float,\n",
    "    use_batch_norm: bool\n",
    ") -> nn.Sequential:\n",
    "    '''Create a CNN with configurable architecture.\n",
    "    \n",
    "    Args:\n",
    "        n_conv_blocks: Number of convolutional blocks (1-4)\n",
    "        initial_filters: Number of filters in first conv layer (doubles each block)\n",
    "        fc_units_1: Number of units in first fully connected layer\n",
    "        fc_units_2: Number of units in second fully connected layer\n",
    "        dropout_rate: Dropout probability\n",
    "        use_batch_norm: Whether to use batch normalization\n",
    "    \n",
    "    Returns:\n",
    "        nn.Sequential model\n",
    "    '''\n",
    "\n",
    "    layers = []\n",
    "    in_channels = 1  # Grayscale input\n",
    "    current_size = 32  # Input image size\n",
    "    \n",
    "    for block_idx in range(n_conv_blocks):\n",
    "        out_channels = initial_filters * (2 ** block_idx)\n",
    "        \n",
    "        # First conv in block\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Second conv in block\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Pooling and dropout\n",
    "        layers.append(nn.MaxPool2d(2, 2))\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        in_channels = out_channels\n",
    "        current_size //= 2\n",
    "    \n",
    "    # Calculate flattened size\n",
    "    final_channels = initial_filters * (2 ** (n_conv_blocks - 1))\n",
    "    flattened_size = final_channels * current_size * current_size\n",
    "    \n",
    "    # Classifier (3 fully connected layers)\n",
    "    layers.append(nn.Flatten())\n",
    "    layers.append(nn.Linear(flattened_size, fc_units_1))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Dropout(dropout_rate))\n",
    "    layers.append(nn.Linear(fc_units_1, fc_units_2))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Dropout(dropout_rate))\n",
    "    layers.append(nn.Linear(fc_units_2, num_classes))\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe334a89",
   "metadata": {},
   "source": [
    "## 3. Optuna hyperparameter optimization\n",
    "\n",
    "We define an objective function that Optuna will optimize. The function:\n",
    "1. Samples hyperparameters from defined search spaces\n",
    "2. Creates and trains a model with those hyperparameters\n",
    "3. Returns the validation accuracy to maximize\n",
    "\n",
    "### 3.1. Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91471137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trial(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    n_epochs: int,\n",
    "    trial: optuna.Trial\n",
    ") -> float:\n",
    "    '''Train a model for a single Optuna trial with pruning support.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        optimizer: Optimizer for training\n",
    "        criterion: Loss function\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        n_epochs: Number of epochs to train\n",
    "        trial: Optuna trial object for reporting and pruning\n",
    "    \n",
    "    Returns:\n",
    "        Best validation accuracy achieved during training\n",
    "    '''\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        best_val_accuracy = max(best_val_accuracy, val_accuracy)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(val_accuracy, epoch)\n",
    "        \n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    '''Optuna objective function for CNN hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object for suggesting hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        Validation accuracy (to maximize)\n",
    "    '''\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    n_conv_blocks = trial.suggest_int('n_conv_blocks', 1, 4)\n",
    "    initial_filters = trial.suggest_categorical('initial_filters', [16, 32, 64])\n",
    "    fc_units_1 = trial.suggest_categorical('fc_units_1', [256, 512, 1024])\n",
    "    fc_units_2 = trial.suggest_categorical('fc_units_2', [64, 128, 256])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.75)\n",
    "    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
    "    \n",
    "    # Create model\n",
    "    model = create_cnn(\n",
    "        n_conv_blocks=n_conv_blocks,\n",
    "        initial_filters=initial_filters,\n",
    "        fc_units_1=fc_units_1,\n",
    "        fc_units_2=fc_units_2,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_batch_norm=use_batch_norm\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    elif optimizer_name == 'SGD':\n",
    "        momentum = trial.suggest_float('sgd_momentum', 0.8, 0.99)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    else:  # RMSprop\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train model and return best validation accuracy\n",
    "    try:\n",
    "        return train_trial(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            n_epochs=n_epochs_per_trial,\n",
    "            trial=trial\n",
    "        )\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "\n",
    "        # Clear CUDA cache and skip this trial\n",
    "        torch.cuda.empty_cache()\n",
    "        raise optuna.TrialPruned(f'CUDA OOM with params: {trial.params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeec9d6",
   "metadata": {},
   "source": [
    "### 3.2. Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeec9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Delete existing study if desired & it exists\n",
    "if start_new_study == True:\n",
    "    try:\n",
    "        optuna.delete_study(study_name='cnn_optimization', storage=storage_url)\n",
    "        print('Deleted existing study')\n",
    "\n",
    "    except KeyError:\n",
    "        print('No existing study found')\n",
    "\n",
    "# Create Optuna study with SQLite storage (maximize validation accuracy)\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='cnn_optimization',\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True,  # Resume if study already exists\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "print(f'Study stored at: {storage_path}')\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "print(f'\\nBest trial:')\n",
    "print(f'  Value (validation accuracy): {study.best_trial.value:.2f}%')\n",
    "print(f'  Params:')\n",
    "\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b4b08",
   "metadata": {},
   "source": [
    "### 3.3. Visualize optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Optimization history\n",
    "axes[0].set_title('Optimization History')\n",
    "\n",
    "# Trial values over time\n",
    "trial_numbers = [t.number for t in study.trials if t.value is not None]\n",
    "trial_values = [t.value for t in study.trials if t.value is not None]\n",
    "\n",
    "axes[0].plot(trial_numbers, trial_values, 'bo-', alpha=0.6)\n",
    "axes[0].axhline(y=study.best_value, color='r', linestyle='--', label=f'Best: {study.best_value:.2f}%')\n",
    "axes[0].set_xlabel('Trial')\n",
    "axes[0].set_ylabel('Validation Accuracy (%)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Hyperparameter importance (if enough trials completed)\n",
    "axes[1].set_title('Hyperparameter Importance')\n",
    "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "if len(completed_trials) >= 5:\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    params = list(importance.keys())\n",
    "    values = list(importance.values())\n",
    "    \n",
    "    axes[1].set_xlabel('Importance')\n",
    "    axes[1].barh(params, values, color='black')\n",
    "\n",
    "else:\n",
    "    axes[1].text(\n",
    "        0.5, 0.5,\n",
    "        'Not enough completed trials\\nfor importance analysis', \n",
    "        ha='center', va='center', transform=axes[1].transAxes\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279fa532",
   "metadata": {},
   "source": [
    "## 4. Train final model with best hyperparameters\n",
    "\n",
    "### 4.1. Create optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "for key, value in best_params.items():\n",
    "    print(f'  {key}: {value}')\n",
    "\n",
    "# Create model with best hyperparameters\n",
    "best_model = create_cnn(\n",
    "    n_conv_blocks=best_params['n_conv_blocks'],\n",
    "    initial_filters=best_params['initial_filters'],\n",
    "    fc_units_1=best_params['fc_units_1'],\n",
    "    fc_units_2=best_params['fc_units_2'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    use_batch_norm=best_params['use_batch_norm']\n",
    ").to(device)\n",
    "\n",
    "# Create optimizer with best hyperparameters\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    best_optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "elif best_params['optimizer'] == 'SGD':\n",
    "    best_optimizer = optim.SGD(\n",
    "        best_model.parameters(), \n",
    "        lr=best_params['learning_rate'],\n",
    "        momentum=best_params.get('sgd_momentum', 0.9)\n",
    "    )\n",
    "\n",
    "else:  # RMSprop\n",
    "    best_optimizer = optim.RMSprop(best_model.parameters(), lr=best_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32158336",
   "metadata": {},
   "source": [
    "### 4.2. Train optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36296863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = train_model(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=best_optimizer,\n",
    "    epochs=n_epochs_final,\n",
    "    print_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80ff5f",
   "metadata": {},
   "source": [
    "### 4.3. Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7be7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_learning_curves(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4dd5b",
   "metadata": {},
   "source": [
    "## 5. Evaluate optimized model on test set\n",
    "\n",
    "### 5.1. Calculate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, predictions, true_labels = evaluate_model(best_model, test_loader)\n",
    "print(f'Test accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba498f",
   "metadata": {},
   "source": [
    "### 5.2. Per-class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "class_correct = {name: 0 for name in class_names}\n",
    "class_total = {name: 0 for name in class_names}\n",
    "\n",
    "for pred, true in zip(predictions, true_labels):\n",
    "\n",
    "    class_name = class_names[true]\n",
    "    class_total[class_name] += 1\n",
    "\n",
    "    if pred == true:\n",
    "        class_correct[class_name] += 1\n",
    "\n",
    "print('Per-class accuracy:')\n",
    "print('-' * 30)\n",
    "\n",
    "for name in class_names:\n",
    "    acc = 100 * class_correct[name] / class_total[name]\n",
    "    print(f'{name:12s}: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c8316",
   "metadata": {},
   "source": [
    "### 5.3. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ef2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_confusion_matrix(true_labels, predictions, class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2a300",
   "metadata": {},
   "source": [
    "### 5.4. Predicted class probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for all test samples\n",
    "best_model.eval()\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        outputs = best_model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "all_probs = np.concatenate(all_probs, axis=0)\n",
    "\n",
    "# Plot probability distributions\n",
    "fig, axes = plot_class_probability_distributions(all_probs, class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10224238",
   "metadata": {},
   "source": [
    "### 5.5. Evaluation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dafb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plot_evaluation_curves(true_labels, all_probs, class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00040f",
   "metadata": {},
   "source": [
    "## 6. Save optimized model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fbf979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('../models/pytorch')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model state dict\n",
    "model_path = models_dir / 'optimized_cnn.pth'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    'optimizer_state_dict': best_optimizer.state_dict(),\n",
    "    'best_params': best_params,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'history': history\n",
    "}, model_path)\n",
    "\n",
    "print(f'Model saved to: {model_path}')\n",
    "print(f'Test accuracy: {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
