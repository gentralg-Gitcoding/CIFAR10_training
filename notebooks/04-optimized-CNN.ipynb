{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8541b05d",
   "metadata": {},
   "source": [
    "# Optimized PyTorch CNN with Optuna\n",
    "\n",
    "In this notebook, we optimize a convolutional neural network (CNN) for the CIFAR-10 dataset using Optuna for hyperparameter tuning.\n",
    "\n",
    "## Notebook set-up\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614cd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Package imports\n",
    "from image_classification_tools.pytorch.data import (\n",
    "    load_datasets, prepare_splits, create_dataloaders\n",
    ")\n",
    "from image_classification_tools.pytorch.evaluation import evaluate_model\n",
    "from image_classification_tools.pytorch.hyperparameter_optimization import create_cnn, create_objective\n",
    "from image_classification_tools.pytorch.plotting import (\n",
    "    plot_sample_images, plot_learning_curves, \n",
    "    plot_confusion_matrix, plot_class_probability_distributions,\n",
    "    plot_evaluation_curves, plot_optimization_results\n",
    ")\n",
    "from image_classification_tools.pytorch.training import train_model\n",
    "\n",
    "# Suppress Optuna info messages (show only warnings and errors)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(315)\n",
    "np.random.seed(315)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077cc69",
   "metadata": {},
   "source": [
    "### Fixed hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna settings\n",
    "run_optimization = True  # Run optimization (True) or load results from disk for evaluation (False)\n",
    "start_new_study = False  # Clear results and start new study (True) or resume previous run (False) \n",
    "n_trials = 200           # Number of optimization trials\n",
    "n_epochs_per_trial = 100 # Epochs per trial\n",
    "n_epochs_final = 150     # Epochs for final model training with optimized hyperparameters\n",
    "print_every = 10         # Print training progress every n epochs\n",
    "\n",
    "# SQLite storage for Optuna (local file)\n",
    "data_dir = Path('../data/pytorch')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "storage_path = data_dir / 'cnn_optimization.db'\n",
    "storage_url = f'sqlite:///{storage_path}'\n",
    "\n",
    "# CIFAR-10 class names in class order\n",
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e56c0",
   "metadata": {},
   "source": [
    "## 1. Visualize CIFAR-10 sample images\n",
    "\n",
    "CIFAR-10 contains 32x32 color images (3 channels) across 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure data directory exists\n",
    "data_dir = Path('../data/pytorch/cifar10')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define transform (RGB)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Get a sample dataset for visualization\n",
    "sample_dataset = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Plot first 10 images from the training dataset\n",
    "fig, axes = plot_sample_images(sample_dataset, class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe334a89",
   "metadata": {},
   "source": [
    "## 2. Optuna hyperparameter optimization\n",
    "\n",
    "We use `create_objective()` to generate an objective function that Optuna will optimize.\n",
    "The function samples hyperparameters, creates and trains a model, and returns validation accuracy.\n",
    "\n",
    "### 2.1. Define hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dffef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space\n",
    "search_space = {\n",
    "    'batch_size': [64, 128, 256, 512],\n",
    "    'n_conv_blocks': (1, 10),              # Number of conv blocks\n",
    "    'initial_filters': [16, 32, 64, 128],  # Filters in first block (doubles each block)\n",
    "    'n_fc_layers': (1, 5),                 # Number of FC layers in classifier\n",
    "    'conv_dropout_rate': (0.1, 0.5),       # Conv block dropout\n",
    "    'fc_dropout_rate': (0.3, 0.7),         # FC layer dropout\n",
    "    'learning_rate': (1e-5, 1e-2, 'log'),\n",
    "    'optimizer': ['Adam', 'SGD', 'RMSprop'],\n",
    "    'sgd_momentum': (0.8, 0.99),           # Used when optimizer='SGD'\n",
    "    'weight_decay': (1e-6, 1e-3, 'log')\n",
    "}\n",
    "\n",
    "print('Hyperparameter search space:')\n",
    "\n",
    "for key, value in search_space.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create objective function for Optuna\n",
    "objective = create_objective(\n",
    "    data_dir=data_dir,\n",
    "    train_transform=transform,\n",
    "    eval_transform=transform,\n",
    "    n_epochs=n_epochs_per_trial,\n",
    "    device=device,\n",
    "    num_classes=num_classes,\n",
    "    search_space=search_space\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cf0e8",
   "metadata": {},
   "source": [
    "### 2.3. Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeec9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if run_optimization:\n",
    "    print('Running hyperparameter optimization...')\n",
    "\n",
    "    # Delete existing study if desired & it exists\n",
    "    if start_new_study == True:\n",
    "\n",
    "        print('Starting new study')\n",
    "        try:\n",
    "            optuna.delete_study(study_name='cnn_optimization', storage=storage_url)\n",
    "            print('Deleted existing study')\n",
    "\n",
    "        except KeyError:\n",
    "            print('No existing study found')\n",
    "\n",
    "    else:\n",
    "        if storage_path.exists():\n",
    "            print(f'Resuming study {storage_path}')\n",
    "\n",
    "        else:\n",
    "            print(f'No prior results found at {storage_path}, starting new study')\n",
    "\n",
    "    # Create Optuna study with SQLite storage (maximize validation accuracy)\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        study_name='cnn_optimization',\n",
    "        storage=storage_url,\n",
    "        load_if_exists=True,  # Resume if study already exists\n",
    "        pruner=None #optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "    )\n",
    "\n",
    "    print(f'Study stored at: {storage_path}')\n",
    "\n",
    "    # Run optimization\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load results from disk\n",
    "    study = optuna.load_study(\n",
    "        study_name='cnn_optimization',\n",
    "        storage=storage_url\n",
    "    )\n",
    "\n",
    "    print(f'Study loaded from: {storage_path}')\n",
    "\n",
    "print(f'\\nBest validation accuracy: {study.best_trial.value:.2f}%')\n",
    "\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f' - {key}: {value}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b4b08",
   "metadata": {},
   "source": [
    "### 2.4. Visualize optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_optimization_results(study)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279fa532",
   "metadata": {},
   "source": [
    "## 3. Train final model with best hyperparameters\n",
    "\n",
    "### 3.1. Create optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyperparameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "for key, value in best_params.items():\n",
    "    print(f'  {key}: {value}')\n",
    "\n",
    "# Recreate data loaders with best batch size\n",
    "best_batch_size = best_params['batch_size']\n",
    "\n",
    "train_loader, val_loader, test_loader = make_data_loaders(\n",
    "    data_dir=data_dir,\n",
    "    batch_size=best_batch_size,\n",
    "    train_transform=transform,\n",
    "    eval_transform=transform,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f'\\nRecreated data loaders with batch size: {best_batch_size}')\n",
    "\n",
    "# Create model with best hyperparameters\n",
    "best_model = create_cnn(\n",
    "    n_conv_blocks=best_params['n_conv_blocks'],\n",
    "    initial_filters=best_params['initial_filters'],\n",
    "    n_fc_layers=best_params['n_fc_layers'],\n",
    "    conv_dropout_rate=best_params['conv_dropout_rate'],\n",
    "    fc_dropout_rate=best_params['fc_dropout_rate'],\n",
    "    num_classes=num_classes,\n",
    "    in_channels=3\n",
    ").to(device)\n",
    "\n",
    "# Create optimizer with best hyperparameters\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    best_optimizer = optim.Adam(\n",
    "        best_model.parameters(), \n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "\n",
    "elif best_params['optimizer'] == 'SGD':\n",
    "    best_optimizer = optim.SGD(\n",
    "        best_model.parameters(), \n",
    "        lr=best_params['learning_rate'],\n",
    "        momentum=best_params.get('sgd_momentum', 0.9),\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "\n",
    "else:  # RMSprop\n",
    "    best_optimizer = optim.RMSprop(\n",
    "        best_model.parameters(), \n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "\n",
    "# Set cross-entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Get total trainable parameters\n",
    "trainable_params = sum(p.numel() for p in best_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'\\n{best_model}')\n",
    "print(f'\\nTotal parameters: {trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32158336",
   "metadata": {},
   "source": [
    "### 3.2. Train optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36296863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = train_model(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=best_optimizer,\n",
    "    device=device,\n",
    "    lazy_loading=False,  # Data already on GPU from make_data_loaders\n",
    "    epochs=n_epochs_final,\n",
    "    print_every=print_every\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80ff5f",
   "metadata": {},
   "source": [
    "### 3.3. Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7be7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_learning_curves(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4dd5b",
   "metadata": {},
   "source": [
    "## 4. Evaluate optimized model on test set\n",
    "\n",
    "### 4.1. Calculate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, predictions, true_labels = evaluate_model(best_model, test_loader)\n",
    "print(f'Test accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba498f",
   "metadata": {},
   "source": [
    "### 4.2. Per-class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "class_correct = {name: 0 for name in class_names}\n",
    "class_total = {name: 0 for name in class_names}\n",
    "\n",
    "for pred, true in zip(predictions, true_labels):\n",
    "\n",
    "    class_name = class_names[true]\n",
    "    class_total[class_name] += 1\n",
    "\n",
    "    if pred == true:\n",
    "        class_correct[class_name] += 1\n",
    "\n",
    "print('Per-class accuracy:')\n",
    "print('-' * 30)\n",
    "\n",
    "for name in class_names:\n",
    "    acc = 100 * class_correct[name] / class_total[name]\n",
    "    print(f'{name:12s}: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c8316",
   "metadata": {},
   "source": [
    "### 4.3. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ef2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_confusion_matrix(true_labels, predictions, class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2a300",
   "metadata": {},
   "source": [
    "### 4.4. Predicted class probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for all test samples\n",
    "best_model.eval()\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        outputs = best_model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "all_probs = np.concatenate(all_probs, axis=0)\n",
    "\n",
    "# Plot probability distributions\n",
    "fig, axes = plot_class_probability_distributions(all_probs, class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10224238",
   "metadata": {},
   "source": [
    "### 4.5. Evaluation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dafb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plot_evaluation_curves(true_labels, all_probs, class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00040f",
   "metadata": {},
   "source": [
    "## 5. Save optimized model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fbf979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('../models/pytorch')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model state dict\n",
    "model_path = models_dir / 'optimized_cnn.pth'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    'optimizer_state_dict': best_optimizer.state_dict(),\n",
    "    'best_params': best_params,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'history': history\n",
    "}, model_path)\n",
    "\n",
    "print(f'Model saved to: {model_path}')\n",
    "print(f'Test accuracy: {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
